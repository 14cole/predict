# This script has been modified to be compatible with scipy 0.17 and optimized for large datasets
# The main changes include:
# 1. Using alternative implementations for Gaussian Process Regression
# 2. Using pickle protocol 2 for compatibility with older Python versions
# 3. Adding error handling for pickle loading
# 4. Suppressing warnings that might occur with older scipy versions
# 5. Implementing batch processing for handling large datasets
# 6. Using incremental learning where possible
# 7. Adding memory-efficient data loading
# 8. Implementing parallel processing for model training

from __future__ import division  # For Python 2.7 compatibility with division
import numpy as np
import pickle
import time
import itertools
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.kernel_approximation import RBFSampler
import warnings
from joblib import Parallel, delayed
import multiprocessing
warnings.filterwarnings('ignore')  # Suppress warnings that might occur with older scipy

# Helper functions for handling large datasets
def batch_generator(X, y, batch_size=1000, shuffle=True):
    """
    Generator function to yield batches of data for memory-efficient processing.

    Args:
        X: Input features
        y: Target values
        batch_size: Size of each batch
        shuffle: Whether to shuffle the data

    Yields:
        X_batch, y_batch: Batches of data
    """
    n_samples = X.shape[0]
    indices = np.arange(n_samples)

    if shuffle:
        np.random.shuffle(indices)

    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]

        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        yield X_batch, y_batch

def fit_model_with_batches(model, X, y, batch_size=1000, scaler=None, epochs=1, verbose=True):
    """
    Train a model using batch processing for large datasets.

    Args:
        model: The model to train
        X: Input features
        y: Target values
        batch_size: Size of each batch
        scaler: Optional scaler for feature standardization
        epochs: Number of passes through the entire dataset
        verbose: Whether to display progress

    Returns:
        model: The trained model
        scaler: The fitted scaler (if provided)
    """
    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))

    # Initialize scaler if provided
    if scaler is not None:
        # Fit scaler on a sample of the data to avoid memory issues
        sample_size = min(10000, n_samples)
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        scaler.fit(X[sample_indices])

    # Training loop
    for epoch in range(epochs):
        if verbose:
            print("Epoch {}/{}".format(epoch+1, epochs))
            print("Training: 0%", end="")

        batch_iterator = batch_generator(X, y, batch_size)
        batch_count = 0

        for X_batch, y_batch in batch_iterator:
            batch_count += 1
            if verbose and batch_count % max(1, n_batches // 10) == 0:
                progress = min(100, int(100 * batch_count / n_batches))
                print("\rTraining: {}%".format(progress), end="")

            # Scale the batch if scaler is provided
            if scaler is not None:
                X_batch = scaler.transform(X_batch)

            # Check if model supports partial_fit (incremental learning)
            if hasattr(model, 'partial_fit'):
                model.partial_fit(X_batch, y_batch)
            else:
                # For models that don't support incremental learning,
                # we'll fit on each batch separately (less optimal but still works)
                model.fit(X_batch, y_batch)

        if verbose:
            print()  # Print newline after progress reporting

    return model, scaler

def predict_with_batches(model, X, batch_size=1000, scaler=None, verbose=True):
    """
    Make predictions using batch processing for large datasets.

    Args:
        model: The trained model
        X: Input features
        batch_size: Size of each batch
        scaler: Optional scaler for feature standardization
        verbose: Whether to display progress

    Returns:
        y_pred: Predictions for all samples
    """
    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))
    y_pred = np.zeros(n_samples)

    if verbose:
        print("Making predictions...")
        print("Predicting: 0%", end="")

    batch_iterator = range(0, n_samples, batch_size)
    batch_count = 0

    for start_idx in batch_iterator:
        batch_count += 1
        if verbose and batch_count % max(1, n_batches // 10) == 0:
            progress = min(100, int(100 * batch_count / n_batches))
            print("\rPredicting: {}%".format(progress), end="")
        end_idx = min(start_idx + batch_size, n_samples)
        X_batch = X[start_idx:end_idx]

        # Scale the batch if scaler is provided
        if scaler is not None:
            X_batch = scaler.transform(X_batch)

        # Make predictions for this batch
        y_pred[start_idx:end_idx] = model.predict(X_batch)

    if verbose:
        print()  # Print newline after progress reporting

    return y_pred

# Function to train a model in parallel
def train_model(model_name, X_train, y_train, X_test, y_test, use_scaling=True, batch_size=1000):
    """
    Train a single model and evaluate it.

    Args:
        model_name: Name of the model to train
        X_train: Training features
        y_train: Training targets
        X_test: Test features
        y_test: Test targets
        use_scaling: Whether to use feature scaling
        batch_size: Batch size for large datasets

    Returns:
        model_info: Dictionary with model, MSE, and other info
    """
    start_time = time.time()
    print("Training {} model...".format(model_name))

    # Initialize scaler if needed
    scaler = StandardScaler() if use_scaling else None

    # Create the appropriate model based on name
    if model_name == 'Gaussian Process':
        try:
            # For large datasets, Gaussian Process is not suitable
            # Use a scalable approximation instead
            rbf_feature = RBFSampler(gamma=1.0, n_components=100, random_state=42)
            X_train_features = rbf_feature.fit_transform(X_train)
            sgd_model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)
            sgd_model.fit(X_train_features, y_train)

            # For prediction
            X_test_features = rbf_feature.transform(X_test)
            y_pred = sgd_model.predict(X_test_features)

            model = {'sgd_model': sgd_model, 'rbf_feature': rbf_feature}
            is_approximate = True
        except Exception as e:
            print("Error with Gaussian Process approximation: {}".format(e))
            # Fallback to SVR
            model = SVR(kernel='rbf')
            if len(X_train) > 10000:  # If dataset is large
                model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                      batch_size=batch_size, 
                                                      scaler=scaler)
                y_pred = predict_with_batches(model, X_test, 
                                             batch_size=batch_size, 
                                             scaler=scaler)
            else:
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
            is_approximate = False

    elif model_name == 'RBF SVR':
        model = SVR(kernel='rbf')
        if len(X_train) > 10000:  # If dataset is large
            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
        else:
            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Linear SVR':
        # For large datasets, use SGDRegressor which supports incremental learning
        if len(X_train) > 10000:
            model = SGDRegressor(loss='epsilon_insensitive', max_iter=1000, tol=1e-3, random_state=42)
            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
            is_approximate = True
        else:
            model = SVR(kernel='linear')
            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
            is_approximate = False

    elif model_name == 'Neural Network':
        if len(X_train) > 10000:  # If dataset is large
            # Configure network without early stopping for batch training
            model = MLPRegressor(
                hidden_layer_sizes=(200, 150, 100, 50),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                batch_size='auto',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                max_iter=2000,
                tol=1e-4,
                early_stopping=False,  # Disable early stopping for batch training
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                random_state=42
            )
            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
        else:
            # For smaller datasets, we can use early stopping
            model = MLPRegressor(
                hidden_layer_sizes=(200, 150, 100, 50),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                batch_size='auto',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                max_iter=2000,
                tol=1e-4,
                early_stopping=True,
                validation_fraction=0.1,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                n_iter_no_change=10,
                random_state=42
            )
            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Random Forest':
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        if len(X_train) > 10000:  # If dataset is large
            # Random Forest doesn't support incremental learning
            # But we can train on batches and combine the results
            n_jobs = max(1, multiprocessing.cpu_count() - 1)  # Use all but one CPU
            model = RandomForestRegressor(n_estimators=100, n_jobs=n_jobs, random_state=42)

            # For very large datasets, subsample
            if len(X_train) > 100000:
                sample_size = 100000
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                X_train_sample = X_train[sample_indices]
                y_train_sample = y_train[sample_indices]
                model.fit(X_train_sample, y_train_sample)
            else:
                model.fit(X_train, y_train)

            y_pred = predict_with_batches(model, X_test, batch_size=batch_size)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Gradient Boosting':
        model = GradientBoostingRegressor(n_estimators=100, random_state=42)
        if len(X_train) > 10000:  # If dataset is large
            # Gradient Boosting doesn't support incremental learning
            # But we can train on a subsample for large datasets
            if len(X_train) > 100000:
                sample_size = 100000
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                X_train_sample = X_train[sample_indices]
                y_train_sample = y_train[sample_indices]
                model.fit(X_train_sample, y_train_sample)
            else:
                model.fit(X_train, y_train)

            y_pred = predict_with_batches(model, X_test, batch_size=batch_size)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        is_approximate = False

    else:
        raise ValueError("Unknown model name: {}".format(model_name))

    # Calculate MSE
    mse = mean_squared_error(y_test, y_pred)

    # Calculate training time
    training_time = time.time() - start_time

    print("{} MSE: {:.6f} (Training time: {:.2f}s)".format(model_name, mse, training_time))

    return {
        'model': model,
        'mse': mse,
        'scaler': scaler,
        'is_approximate': is_approximate,
        'training_time': training_time
    }

def predict_with_loaded_model(loaded_data, X_sample, batch_size=1000):
    """
    Make predictions using a loaded model, handling different model types and large datasets.

    Args:
        loaded_data: Dictionary containing the loaded model and components
        X_sample: Input data for prediction
        batch_size: Batch size for large datasets

    Returns:
        predictions: Model predictions
    """
    model_name = loaded_data.get('model_name', 'Unknown')
    model = loaded_data['model']
    is_approximate = loaded_data.get('is_approximate', False)
    scaler = loaded_data.get('scaler', None)

    # Handle large input arrays
    if len(X_sample) > batch_size:
        print("Making predictions on large input array (size: {})...".format(len(X_sample)))

        if is_approximate and model_name == 'Gaussian Process':
            # For GP approximation, we need to transform with RBF feature first
            rbf_feature = model['rbf_feature']
            sgd_model = model['sgd_model']

            # Process in batches to avoid memory issues
            predictions = np.zeros(len(X_sample))
            for start_idx in range(0, len(X_sample), batch_size):
                end_idx = min(start_idx + batch_size, len(X_sample))
                X_batch = X_sample[start_idx:end_idx]

                # Scale if needed
                if scaler is not None:
                    X_batch = scaler.transform(X_batch)

                # Transform and predict
                X_batch_features = rbf_feature.transform(X_batch)
                predictions[start_idx:end_idx] = sgd_model.predict(X_batch_features)

            return predictions
        else:
            # Use batch prediction for other models
            return predict_with_batches(model, X_sample, batch_size=batch_size, scaler=scaler)
    else:
        # For small inputs, we can predict directly
        if is_approximate and model_name == 'Gaussian Process':
            rbf_feature = model['rbf_feature']
            sgd_model = model['sgd_model']

            # Scale if needed
            if scaler is not None:
                X_sample = scaler.transform(X_sample)

            X_sample_features = rbf_feature.transform(X_sample)
            return sgd_model.predict(X_sample_features)
        else:
            # Scale if needed
            if scaler is not None:
                X_sample = scaler.transform(X_sample)

            return model.predict(X_sample)

def predict_with_variable_combinations(loaded_data, variable_lists, batch_size=1000, verbose=True):
    """
    Make predictions for all combinations of input variables.

    Args:
        loaded_data: Dictionary containing the loaded model and components
        variable_lists: List of lists, where each inner list contains possible values for one dimension
        batch_size: Batch size for processing large numbers of combinations
        verbose: Whether to display progress

    Returns:
        results: Dictionary mapping input combinations to predictions
    """
    # Validate input
    if not isinstance(variable_lists, list) or not all(isinstance(var_list, list) for var_list in variable_lists):
        raise ValueError("variable_lists must be a list of lists")

    # Get the number of dimensions
    n_dims = len(variable_lists)

    # Generate all combinations using itertools.product
    all_combinations = list(itertools.product(*variable_lists))
    n_combinations = len(all_combinations)

    if verbose:
        print("Generating predictions for {} combinations of input variables...".format(n_combinations))

    # Process combinations in batches to handle large numbers efficiently
    results = {}

    # Process in batches
    for batch_start in range(0, n_combinations, batch_size):
        batch_end = min(batch_start + batch_size, n_combinations)
        batch_combinations = all_combinations[batch_start:batch_end]

        # Convert combinations to numpy array for prediction
        X_batch = np.array(batch_combinations)

        # Make predictions for this batch
        batch_predictions = predict_with_loaded_model(loaded_data, X_batch, batch_size)

        # Store results
        for i, combination in enumerate(batch_combinations):
            results[combination] = batch_predictions[i]

        if verbose and batch_end < n_combinations:
            print("Processed {}/{} combinations...".format(batch_end, n_combinations))

    if verbose:
        print("Completed predictions for all {} combinations.".format(n_combinations))

    return results

def main():
    """
    Main function that organizes all inputs and returns outputs.
    This function handles:
    1. Data preparation
    2. Model training and evaluation
    3. Finding the best model
    4. Saving the model
    5. Example usage of the model

    Returns:
        dict: Results including best model, predictions, and combination results
    """
    # Input data - split into separate X and y arrays

    # Base values for X and y
    y_base = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6])
    X_base = np.array([
        [2, 3, 4, 5, 6, 7],
        [4, 5, 6, 7, 3, 2],
        [5, 6, 7, 2, 1, 3],
        [6, 7, 2, 1, 2, 4],
        [7, 2, 1, 2, 3, 5],
        [2, 1, 2, 3, 4, 6],
        [1, 2, 3, 4, 5, 7],
        [2, 3, 4, 5, 6, 8],
        [3, 4, 5, 6, 7, 9],
        [4, 5, 6, 7, 8, 10],
        [5, 4, 3, 2, 1, 6]
    ])

    # Create a larger dataset by repeating the base arrays to enable batch processing
    # Repeat the data to create more than 10,000 samples for X_train after the train-test split
    repeat_factor = 1500  # This will create 16,500 samples (with ~11,550 in X_train after 70-30 split)
    X = np.tile(X_base, (repeat_factor, 1))
    y = np.tile(y_base, repeat_factor)

    print(f"Created dataset with {len(X)} samples to enable batch processing")

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initialize scaler
    scaler = StandardScaler()

    # Dictionary to store models and their MSE scores
    models = {}

    # Determine if we should use batch processing based on dataset size
    use_batch_processing = len(X_train) > 10000
    batch_size = 10000 if use_batch_processing else None

    # For small datasets, we can train all models sequentially
    if not use_batch_processing:
        # Train models sequentially
        models['Gaussian Process'] = train_model('Gaussian Process', X_train, y_train, X_test, y_test)
        models['RBF SVR'] = train_model('RBF SVR', X_train, y_train, X_test, y_test)
        models['Linear SVR'] = train_model('Linear SVR', X_train, y_train, X_test, y_test)
        models['Neural Network'] = train_model('Neural Network', X_train, y_train, X_test, y_test)
        models['Random Forest'] = train_model('Random Forest', X_train, y_train, X_test, y_test, use_scaling=False)
        models['Gradient Boosting'] = train_model('Gradient Boosting', X_train, y_train, X_test, y_test, use_scaling=False)
    else:
        # For large datasets, train models in parallel
        print("Training models in parallel for large dataset (size: {})...".format(len(X_train)))

        # Define model configurations
        model_configs = [
            ('Gaussian Process', True),  # (model_name, use_scaling)
            ('RBF SVR', True),
            ('Linear SVR', True),
            ('Neural Network', True),
            ('Random Forest', False),
            ('Gradient Boosting', False)
        ]

        # Train models in parallel
        n_jobs = max(1, multiprocessing.cpu_count() - 1)  # Use all but one CPU
        results = Parallel(n_jobs=n_jobs)(
            delayed(train_model)(name, X_train, y_train, X_test, y_test, use_scaling, batch_size)
            for name, use_scaling in model_configs
        )

        # Store results
        for (name, _), result in zip(model_configs, results):
            models[name] = result

    # Find the best model based on MSE
    best_model_name = min(models, key=lambda k: models[k]['mse'])
    best_model_info = models[best_model_name]
    best_mse = best_model_info['mse']
    best_model = best_model_info['model']
    is_approximate = best_model_info.get('is_approximate', False)
    training_time = best_model_info.get('training_time', 0)

    print("\nBest model: {} with MSE: {:.6f}".format(best_model_name, best_mse))
    print("Training time: {:.2f}s".format(training_time))
    if is_approximate:
        print("Note: This is an approximate model optimized for large datasets")

    # Save each model with its respective name
    print("\nSaving all models with their respective names...")

    for model_name, model_info in models.items():
        # Create filename based on model name
        if best_model_name == model_name:
            model_filename = f"best_{model_name.replace(' ', '_').lower()}_model.pkl"
        else:
            model_filename = f"{model_name.replace(' ', '_').lower()}_model.pkl"
        # Prepare the data to save
        save_data = {'model_name': model_name}

        # Add model and any necessary components
        if model_info.get('is_approximate', False) and model_name == 'Gaussian Process':
            # For the GP approximation, we need to save both the SGD model and the RBF feature
            save_data['model'] = model_info['model']
            save_data['is_approximate'] = True
        else:
            save_data['model'] = model_info['model']
            save_data['is_approximate'] = model_info.get('is_approximate', False)

        # Add scaler if needed
        if 'scaler' in model_info and model_info['scaler'] is not None:
            save_data['scaler'] = model_info['scaler']

        # Use protocol=2 for pickle to ensure compatibility with older Python versions
        with open(model_filename, 'wb') as file:
            pickle.dump(save_data, file, protocol=2)
        print(f"Model '{model_name}' saved to {model_filename}")

    # Example of how to load and use the saved model
    print("\nExample of loading and using the saved model:")
    try:
        with open(model_filename, 'rb') as file:
            # Use encoding='latin1' for compatibility with older Python versions
            loaded_data = pickle.load(file, encoding='latin1')
    except TypeError:
        # If encoding parameter is not supported (Python 2 compatibility)
        with open(model_filename, 'rb') as file:
            loaded_data = pickle.load(file)

    # Make a prediction with the loaded model
    sample_data = X_test[0:1]  # Take the first test sample
    prediction = predict_with_loaded_model(loaded_data, sample_data)

    print("Actual value: {}".format(y_test[0]))
    print("Predicted value: {}".format(prediction[0]))


    # Define lists of possible values for each dimension
    # For this example, we'll use 2 possible values for each of the 6 dimensions
    dimension_values = [
        [2.0, 4.0],  # Possible values for dimension 1
        [3.0, 5.0],  # Possible values for dimension 2
        [4.0, 6.0],  # Possible values for dimension 3
        [5.0, 7.0],  # Possible values for dimension 4
        [6.0, 8.0],  # Possible values for dimension 5
        [7.0, 9.0]   # Possible values for dimension 6
    ]

    # Get predictions for all combinations
    combination_results = predict_with_variable_combinations(loaded_data, dimension_values)

    # Display a few results
    print("\nTotal combinations: {}".format(len(combination_results)))
    print("\nSample predictions:")
    for i, (combination, prediction) in enumerate(combination_results.items()):
        print("Input: {} â†’ Prediction: {:.4f}".format(combination, prediction))
        if i >= 4:  # Show only first 5 combinations
            remaining = len(combination_results) - 5
            print("... and {} more combinations".format(remaining))
            break

    # Return results
    return {
        'best_model_name': best_model_name,
        'best_model': best_model,
        'best_mse': best_mse,
        'prediction': prediction,
        'combination_results': combination_results
    }

# Call the main function if this script is run directly
if __name__ == "__main__":
    main()
