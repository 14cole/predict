# This script has been modified to be compatible with scipy 0.17 and optimized for large datasets
# The main changes include:
# 1. Using alternative implementations for Gaussian Process Regression
# 2. Using pickle protocol 2 for compatibility with older Python versions
# 3. Adding error handling for pickle loading
# 4. Suppressing warnings that might occur with older scipy versions
# 5. Implementing batch processing for handling large datasets
# 6. Using incremental learning where possible
# 7. Adding memory-efficient data loading
# 8. Implementing parallel processing for model training
# 9. Supporting complex numbers (real and imaginary parts) with phase preservation

from __future__ import division  # For Python 2.7 compatibility with division
import numpy as np
import pickle
import time
import itertools
from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
from sklearn.kernel_approximation import RBFSampler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel
import warnings
from joblib import Parallel, delayed
import multiprocessing
import math
warnings.filterwarnings('ignore')  # Suppress warnings that might occur with older scipy

# Helper functions for handling complex data
def split_complex_data(X_complex, y_complex):
    """
    Split complex data into real and imaginary parts.

    Args:
        X_complex: Input features with complex values
        y_complex: Target values with complex values

    Returns:
        X_real: Real part of input features
        X_imag: Imaginary part of input features
        y_real: Real part of target values
        y_imag: Imaginary part of target values
        y_phase: Phase of target values (for phase preservation)
    """
    # Extract real and imaginary parts of input features
    X_real = np.real(X_complex)
    X_imag = np.imag(X_complex)

    # Extract real and imaginary parts of target values
    y_real = np.real(y_complex)
    y_imag = np.imag(y_complex)

    # Calculate phase of target values (for phase preservation)
    y_phase = np.angle(y_complex)

    return X_real, X_imag, y_real, y_imag, y_phase

def combine_complex_predictions(y_real_pred, y_imag_pred, y_phase=None):
    """
    Combine real and imaginary predictions into complex predictions,
    optionally preserving the original phase.

    Args:
        y_real_pred: Predicted real part
        y_imag_pred: Predicted imaginary part
        y_phase: Original phase to preserve (if None, use predicted phase)

    Returns:
        y_complex_pred: Complex predictions with preserved phase
    """
    # Calculate magnitude from real and imaginary predictions
    magnitude = np.sqrt(y_real_pred**2 + y_imag_pred**2)

    if y_phase is not None:
        # Use the original phase for phase preservation
        y_complex_pred = magnitude * np.exp(1j * y_phase)
    else:
        # Calculate phase from real and imaginary predictions
        phase = np.arctan2(y_imag_pred, y_real_pred)
        y_complex_pred = magnitude * np.exp(1j * phase)

    return y_complex_pred

def train_complex_model(model_name, X_complex_train, y_complex_train, X_complex_test, y_complex_test,
                        use_scaling=True, batch_size=1000, preserve_phase=True, optimize=True):
    """
    Train separate models for real and imaginary parts of complex data.

    Args:
        model_name: Name of the model to train
        X_complex_train: Complex training features
        y_complex_train: Complex training targets
        X_complex_test: Complex test features
        y_complex_test: Complex test targets
        use_scaling: Whether to use feature scaling
        batch_size: Batch size for large datasets
        preserve_phase: Whether to preserve the original phase
        optimize: Whether to optimize the number of iterations or estimators for the model

    Returns:
        model_info: Dictionary with models, MSE, and other info
    """
    start_time = time.time()
    print("Training complex {} model...".format(model_name))

    # Split complex data into real and imaginary parts
    X_real_train, X_imag_train, y_real_train, y_imag_train, y_phase_train = split_complex_data(
        X_complex_train, y_complex_train)
    X_real_test, X_imag_test, y_real_test, y_imag_test, y_phase_test = split_complex_data(
        X_complex_test, y_complex_test)

    # Train model for real part
    print("Training model for real part...")
    real_model_info = train_model(model_name, X_real_train, y_real_train, X_real_test, y_real_test,
                                 use_scaling, batch_size, optimize)

    # Train model for imaginary part
    print("Training model for imaginary part...")
    imag_model_info = train_model(model_name, X_imag_train, y_imag_train, X_imag_test, y_imag_test,
                                 use_scaling, batch_size, optimize)

    # Make predictions
    y_real_pred = predict_with_batches(real_model_info['model'], X_real_test,
                                      batch_size, real_model_info.get('scaler'))
    y_imag_pred = predict_with_batches(imag_model_info['model'], X_imag_test,
                                      batch_size, imag_model_info.get('scaler'))

    # Combine real and imaginary predictions
    if preserve_phase:
        y_complex_pred = combine_complex_predictions(y_real_pred, y_imag_pred, y_phase_test)
    else:
        y_complex_pred = combine_complex_predictions(y_real_pred, y_imag_pred)

    # Calculate MSE for complex predictions
    mse = np.mean(np.abs(y_complex_test - y_complex_pred)**2)

    # Calculate training time
    training_time = time.time() - start_time

    print("Complex {} MSE: {:.6f} (Training time: {:.2f}s)".format(model_name, mse, training_time))

    return {
        'real_model': real_model_info['model'],
        'imag_model': imag_model_info['model'],
        'real_scaler': real_model_info.get('scaler'),
        'imag_scaler': imag_model_info.get('scaler'),
        'is_complex': True,
        'preserve_phase': preserve_phase,
        'mse': mse,
        'training_time': training_time
    }

def predict_with_complex_model(model_info, X_data, batch_size=1000):
    """
    Make predictions using a complex model (separate models for real and imaginary parts).

    Args:
        model_info: Dictionary containing the model components
        X_data: Input data for prediction (can be complex or real)
        batch_size: Batch size for large datasets

    Returns:
        y_pred: Predictions (complex or real depending on the model)
    """
    # Check if the model is complex
    is_complex = model_info.get('is_complex', True)  # Default to True for backward compatibility

    if is_complex:
        # For complex models, split input into real and imaginary parts
        X_real = np.real(X_data)
        X_imag = np.imag(X_data)

        # Get real and imaginary models and scalers
        real_model = model_info['real_model']
        imag_model = model_info['imag_model']
        real_scaler = model_info.get('real_scaler')
        imag_scaler = model_info.get('imag_scaler')
        preserve_phase = model_info.get('preserve_phase', True)

        # Make predictions for real and imaginary parts
        y_real_pred = predict_with_batches(real_model, X_real, batch_size, real_scaler, verbose=False)
        y_imag_pred = predict_with_batches(imag_model, X_imag, batch_size, imag_scaler, verbose=False)

        # Combine real and imaginary predictions
        # Note: For prediction, we don't have the original phase, so we use the predicted phase
        y_pred = combine_complex_predictions(y_real_pred, y_imag_pred)
    else:
        # For real models, use the model directly
        model = model_info['model']
        scaler = model_info.get('scaler')

        # Make predictions
        y_pred = predict_with_batches(model, X_data, batch_size, scaler, verbose=False)

    return y_pred

# Helper functions for handling large datasets
def batch_generator(X, y, batch_size=1000, shuffle=True):
    """
    Generator function to yield batches of data for memory-efficient processing.

    Args:
        X: Input features
        y: Target values
        batch_size: Size of each batch
        shuffle: Whether to shuffle the data

    Yields:
        X_batch, y_batch: Batches of data
    """
    n_samples = X.shape[0]
    indices = np.arange(n_samples)

    if shuffle:
        np.random.shuffle(indices)

    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]

        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        yield X_batch, y_batch

def fit_model_with_batches(model, X, y, batch_size=1000, scaler=None, epochs=1, verbose=True):
    """
    Train a model using batch processing for large datasets.

    Args:
        model: The model to train
        X: Input features
        y: Target values
        batch_size: Size of each batch
        scaler: Optional scaler for feature standardization
        epochs: Number of passes through the entire dataset
        verbose: Whether to display progress

    Returns:
        model: The trained model
        scaler: The fitted scaler (if provided)
    """
    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))

    # Initialize scaler if provided
    if scaler is not None:
        # Fit scaler on a sample of the data to avoid memory issues
        sample_size = min(10000, n_samples)
        sample_indices = np.random.choice(n_samples, sample_size, replace=False)
        scaler.fit(X[sample_indices])

    # Training loop
    for epoch in range(epochs):
        if verbose:
            print("Epoch {}/{}".format(epoch+1, epochs))
            print("Training: 0%", end="")

        batch_iterator = batch_generator(X, y, batch_size)
        batch_count = 0

        for X_batch, y_batch in batch_iterator:
            batch_count += 1
            if verbose and batch_count % max(1, n_batches // 10) == 0:
                progress = min(100, int(100 * batch_count / n_batches))
                print("\rTraining: {}%".format(progress), end="")

            # Scale the batch if scaler is provided
            if scaler is not None:
                X_batch = scaler.transform(X_batch)

            # Check if model supports partial_fit (incremental learning)
            if hasattr(model, 'partial_fit'):
                model.partial_fit(X_batch, y_batch)
            else:
                # For models that don't support incremental learning,
                # we'll fit on each batch separately (less optimal but still works)
                model.fit(X_batch, y_batch)

        if verbose:
            print()  # Print newline after progress reporting

    return model, scaler

def predict_with_batches(model, X, batch_size=1000, scaler=None, verbose=True):
    """
    Make predictions using batch processing for large datasets.

    Args:
        model: The trained model
        X: Input features
        batch_size: Size of each batch
        scaler: Optional scaler for feature standardization
        verbose: Whether to display progress

    Returns:
        y_pred: Predictions for all samples
    """
    n_samples = X.shape[0]
    n_batches = int(np.ceil(n_samples / batch_size))
    y_pred = np.zeros(n_samples)

    if verbose:
        print("Making predictions...")
        print("Predicting: 0%", end="")

    batch_iterator = range(0, n_samples, batch_size)
    batch_count = 0

    for start_idx in batch_iterator:
        batch_count += 1
        if verbose and batch_count % max(1, n_batches // 10) == 0:
            progress = min(100, int(100 * batch_count / n_batches))
            print("\rPredicting: {}%".format(progress), end="")
        end_idx = min(start_idx + batch_size, n_samples)
        X_batch = X[start_idx:end_idx]

        # Scale the batch if scaler is provided
        if scaler is not None:
            # Check if the scaler is fitted before using transform
            if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                X_batch = scaler.transform(X_batch)
            else:
                # If scaler is not fitted, fit it on this batch first
                X_batch = scaler.fit_transform(X_batch)

        # Make predictions for this batch
        y_pred[start_idx:end_idx] = model.predict(X_batch)

    if verbose:
        print()  # Print newline after progress reporting

    return y_pred

# Function to train a model in parallel
def optimize_iterations(model, param_name, param_range, X_train, y_train, cv=3, n_iter=10, verbose=True):
    """
    Optimize the number of iterations or estimators for a given model.

    Args:
        model: The model to optimize
        param_name: The name of the parameter to optimize (e.g., 'max_iter', 'n_estimators')
        param_range: List of parameter values to try
        X_train: Training features
        y_train: Training targets
        cv: Number of cross-validation folds
        n_iter: Number of parameter settings to try
        verbose: Whether to display progress

    Returns:
        best_model: The model with the optimal parameter value
        best_param: The optimal parameter value
    """
    if verbose:
        print(f"Optimizing {param_name} for {model.__class__.__name__}...")

    # Create parameter grid
    param_grid = {param_name: param_range}

    # For large datasets, use a smaller number of CV folds
    if len(X_train) > 10000:
        cv = min(cv, 2)

    # Use RandomizedSearchCV for efficiency
    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grid,
        n_iter=n_iter,
        cv=cv,
        scoring='neg_mean_squared_error',
        n_jobs=-1,  # Use all available cores
        random_state=42,
        verbose=0
    )

    # Fit the random search
    random_search.fit(X_train, y_train)

    # Get the best parameter and model
    best_param = random_search.best_params_[param_name]
    best_score = -random_search.best_score_  # Convert back to MSE

    if verbose:
        print(f"Best {param_name}: {best_param} (MSE: {best_score:.6f})")

    # Return the best model and parameter
    return random_search.best_estimator_, best_param

def train_model(model_name, X_train, y_train, X_test, y_test, use_scaling=True, batch_size=1000, optimize=True):
    """
    Train a single model and evaluate it.

    Args:
        model_name: Name of the model to train
        X_train: Training features
        y_train: Training targets
        X_test: Test features
        y_test: Test targets
        use_scaling: Whether to use feature scaling
        batch_size: Batch size for large datasets
        optimize: Whether to optimize the number of iterations or estimators for the model

    Returns:
        model_info: Dictionary with model, MSE, and other info
    """
    start_time = time.time()
    print("Training {} model...".format(model_name))

    # Initialize scaler if needed
    scaler = StandardScaler() if use_scaling else None

    # Create the appropriate model based on name
    if model_name == 'Gaussian Process':
        # Define a threshold for using exact GP vs approximation
        GP_THRESHOLD = 5000  # Use exact GP for datasets smaller than this

        try:
            if len(X_train) <= GP_THRESHOLD:
                # For smaller datasets, use the exact Gaussian Process implementation
                print(f"Using exact Gaussian Process (dataset size: {len(X_train)})")

                # Apply scaling if requested
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    X_train_for_gp = X_train_scaled
                    X_test_for_gp = X_test_scaled
                else:
                    X_train_for_gp = X_train
                    X_test_for_gp = X_test

                # Define the kernel: combination of RBF, ConstantKernel and WhiteKernel for noise
                kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))

                # Create and fit the Gaussian Process model
                gp_model = GaussianProcessRegressor(
                    kernel=kernel,
                    alpha=1e-10,  # Numerical stability
                    optimizer='fmin_l_bfgs_b',  # Optimizer for kernel parameters
                    n_restarts_optimizer=3,  # Number of restarts for optimization
                    normalize_y=True,  # Normalize target values
                    copy_X_train=False,  # Save memory
                    random_state=42
                )

                # Fit the model
                gp_model.fit(X_train_for_gp, y_train)

                # Make predictions
                y_pred = gp_model.predict(X_test_for_gp)

                # Store the model
                model = gp_model
                is_approximate = False

            else:
                # For larger datasets, use a scalable approximation
                print(f"Using Gaussian Process approximation (dataset size: {len(X_train)})")

                # Apply scaling if requested
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    X_train_for_approx = X_train_scaled
                    X_test_for_approx = X_test_scaled
                else:
                    X_train_for_approx = X_train
                    X_test_for_approx = X_test

                # Use Random Fourier Features approximation
                n_components = min(500, len(X_train) // 10)  # Adaptive number of components
                rbf_feature = RBFSampler(
                    gamma=0.1,  # Inverse of length scale
                    n_components=n_components,
                    random_state=42
                )

                # Transform the data
                X_train_features = rbf_feature.fit_transform(X_train_for_approx)

                # Use SGDRegressor for incremental learning
                sgd_model = SGDRegressor(
                    loss='squared_error',
                    penalty='l2',
                    alpha=0.0001,
                    max_iter=1000,  # Default value, will be optimized if optimize=True
                    tol=1e-3,
                    shuffle=True,
                    learning_rate='optimal',
                    eta0=0.01,
                    random_state=42
                )

                # Optimize max_iter if requested
                if optimize:
                    # Define range of max_iter values to try
                    max_iter_range = [500, 1000, 1500, 2000, 2500, 3000]

                    # Use a subset of data for optimization if dataset is very large
                    if len(X_train) > 50000:
                        sample_size = 50000
                        sample_indices = np.random.choice(len(X_train_features), sample_size, replace=False)
                        X_opt = X_train_features[sample_indices]
                        y_opt = y_train[sample_indices]
                    else:
                        X_opt = X_train_features
                        y_opt = y_train

                    # Optimize max_iter
                    sgd_model, best_max_iter = optimize_iterations(
                        sgd_model, 'max_iter', max_iter_range, X_opt, y_opt
                    )
                    print(f"Using optimized max_iter={best_max_iter} for Gaussian Process approximation")
                else:
                    # Fit the model with default parameters
                    sgd_model.fit(X_train_features, y_train)

                # Make predictions
                X_test_features = rbf_feature.transform(X_test_for_approx)
                y_pred = sgd_model.predict(X_test_features)

                # Create a class to wrap the model components with a predict method
                class GaussianProcessApproximation:
                    def __init__(self, sgd_model, rbf_feature, scaler=None, use_scaling=False):
                        self.sgd_model = sgd_model
                        self.rbf_feature = rbf_feature
                        self.scaler = scaler
                        self.use_scaling = use_scaling

                    def predict(self, X):
                        # Apply scaling if needed
                        if self.use_scaling and self.scaler is not None:
                            if hasattr(self.scaler, 'n_features_in_') or hasattr(self.scaler, 'n_samples_seen_'):
                                X = self.scaler.transform(X)
                            else:
                                X = self.scaler.fit_transform(X)

                        # Transform and predict
                        X_features = self.rbf_feature.transform(X)
                        return self.sgd_model.predict(X_features)

                # Store the model
                model = GaussianProcessApproximation(sgd_model, rbf_feature, scaler, use_scaling)
                is_approximate = True

        except Exception as e:
            print("Error with Gaussian Process: {}".format(e))
            print("Falling back to SVR with RBF kernel")

            # Fallback to SVR with RBF kernel
            model = SVR(kernel='rbf', C=1.0, epsilon=0.1)

            if len(X_train) > 10000:  # If dataset is large
                model, scaler = fit_model_with_batches(model, X_train, y_train,
                                                      batch_size=batch_size,
                                                      scaler=scaler)
                y_pred = predict_with_batches(model, X_test,
                                             batch_size=batch_size,
                                             scaler=scaler)
            else:
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_test_scaled = scaler.transform(X_test)
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

            is_approximate = False

    elif model_name == 'RBF SVR':
        model = SVR(kernel='rbf')
        if len(X_train) > 10000:  # If dataset is large
            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
        else:
            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Linear SVR':
        # For large datasets, use SGDRegressor which supports incremental learning
        if len(X_train) > 10000:
            # Initialize model with default parameters
            model = SGDRegressor(
                loss='epsilon_insensitive', 
                max_iter=1000,  # Default value, will be optimized if optimize=True
                tol=1e-3, 
                random_state=42
            )

            # Optimize max_iter if requested
            if optimize:
                # Apply scaling for optimization
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                else:
                    X_train_scaled = X_train

                # Define range of max_iter values to try
                max_iter_range = [500, 1000, 1500, 2000, 2500, 3000]

                # Use a subset of data for optimization if dataset is very large
                if len(X_train) > 50000:
                    sample_size = 50000
                    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                    X_opt = X_train_scaled[sample_indices]
                    y_opt = y_train[sample_indices]
                else:
                    X_opt = X_train_scaled
                    y_opt = y_train

                # Optimize max_iter
                model, best_max_iter = optimize_iterations(
                    model, 'max_iter', max_iter_range, X_opt, y_opt
                )
                print(f"Using optimized max_iter={best_max_iter} for Linear SVR")

            # Train the model with batches
            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
            is_approximate = True
        else:
            model = SVR(kernel='linear')
            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
            is_approximate = False

    elif model_name == 'Neural Network':
        if len(X_train) > 10000:  # If dataset is large
            # Configure network without early stopping for batch training
            model = MLPRegressor(
                hidden_layer_sizes=(200, 150, 100, 50),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                batch_size='auto',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                max_iter=2000,  # Default value, will be optimized if optimize=True
                tol=1e-4,
                early_stopping=False,  # Disable early stopping for batch training
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                random_state=42
            )

            # Optimize max_iter if requested
            if optimize:
                # Apply scaling for optimization
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                else:
                    X_train_scaled = X_train

                # Define range of max_iter values to try
                max_iter_range = [500, 1000, 1500, 2000, 2500, 3000]

                # Use a subset of data for optimization if dataset is very large
                if len(X_train) > 50000:
                    sample_size = 50000
                    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                    X_opt = X_train_scaled[sample_indices]
                    y_opt = y_train[sample_indices]
                else:
                    X_opt = X_train_scaled
                    y_opt = y_train

                # Optimize max_iter
                model, best_max_iter = optimize_iterations(
                    model, 'max_iter', max_iter_range, X_opt, y_opt
                )
                print(f"Using optimized max_iter={best_max_iter} for Neural Network (large dataset)")

            model, scaler = fit_model_with_batches(model, X_train, y_train, 
                                                  batch_size=batch_size, 
                                                  scaler=scaler)
            y_pred = predict_with_batches(model, X_test, 
                                         batch_size=batch_size, 
                                         scaler=scaler)
        else:
            # For smaller datasets, we can use early stopping
            model = MLPRegressor(
                hidden_layer_sizes=(200, 150, 100, 50),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                batch_size='auto',
                learning_rate='adaptive',
                learning_rate_init=0.001,
                max_iter=2000,  # Default value, will be optimized if optimize=True
                tol=1e-4,
                early_stopping=True,
                validation_fraction=0.1,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                n_iter_no_change=10,
                random_state=42
            )

            # Optimize max_iter if requested
            if optimize:
                # Apply scaling for optimization
                if use_scaling:
                    X_train_scaled = scaler.fit_transform(X_train)
                else:
                    X_train_scaled = X_train

                # Define range of max_iter values to try
                max_iter_range = [500, 1000, 1500, 2000, 2500, 3000]

                # Optimize max_iter
                model, best_max_iter = optimize_iterations(
                    model, 'max_iter', max_iter_range, X_train_scaled, y_train
                )
                print(f"Using optimized max_iter={best_max_iter} for Neural Network (small dataset)")

            if use_scaling:
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Random Forest':
        # Initialize with default parameters
        n_jobs = max(1, multiprocessing.cpu_count() - 1)  # Use all but one CPU
        model = RandomForestRegressor(n_estimators=100, n_jobs=n_jobs, random_state=42)

        if len(X_train) > 10000:  # If dataset is large
            # Random Forest doesn't support incremental learning
            # But we can train on batches and combine the results

            # Optimize n_estimators if requested
            if optimize:
                # For very large datasets, use a subsample for optimization
                if len(X_train) > 100000:
                    sample_size = 100000
                    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                    X_opt = X_train[sample_indices]
                    y_opt = y_train[sample_indices]
                else:
                    X_opt = X_train
                    y_opt = y_train

                # Define range of n_estimators values to try
                n_estimators_range = [50, 100, 150, 200, 250, 300]

                # Optimize n_estimators
                model, best_n_estimators = optimize_iterations(
                    model, 'n_estimators', n_estimators_range, X_opt, y_opt
                )
                print(f"Using optimized n_estimators={best_n_estimators} for Random Forest")

            # For very large datasets, subsample for final training
            if len(X_train) > 100000:
                sample_size = 100000
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                X_train_sample = X_train[sample_indices]
                y_train_sample = y_train[sample_indices]
                model.fit(X_train_sample, y_train_sample)
            else:
                model.fit(X_train, y_train)

            y_pred = predict_with_batches(model, X_test, batch_size=batch_size)
        else:
            # For smaller datasets, we can optimize and train on the full dataset
            if optimize:
                # Define range of n_estimators values to try
                n_estimators_range = [50, 100, 150, 200, 250, 300]

                # Optimize n_estimators
                model, best_n_estimators = optimize_iterations(
                    model, 'n_estimators', n_estimators_range, X_train, y_train
                )
                print(f"Using optimized n_estimators={best_n_estimators} for Random Forest")

            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        is_approximate = False

    elif model_name == 'Gradient Boosting':
        # Initialize with default parameters
        model = GradientBoostingRegressor(n_estimators=100, random_state=42)

        if len(X_train) > 10000:  # If dataset is large
            # Gradient Boosting doesn't support incremental learning
            # But we can train on a subsample for large datasets

            # Optimize n_estimators if requested
            if optimize:
                # For very large datasets, use a subsample for optimization
                if len(X_train) > 100000:
                    sample_size = 100000
                    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                    X_opt = X_train[sample_indices]
                    y_opt = y_train[sample_indices]
                else:
                    X_opt = X_train
                    y_opt = y_train

                # Define range of n_estimators values to try
                n_estimators_range = [50, 100, 150, 200, 250, 300]

                # Optimize n_estimators
                model, best_n_estimators = optimize_iterations(
                    model, 'n_estimators', n_estimators_range, X_opt, y_opt
                )
                print(f"Using optimized n_estimators={best_n_estimators} for Gradient Boosting")

            # For very large datasets, subsample for final training
            if len(X_train) > 100000:
                sample_size = 100000
                sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
                X_train_sample = X_train[sample_indices]
                y_train_sample = y_train[sample_indices]
                model.fit(X_train_sample, y_train_sample)
            else:
                model.fit(X_train, y_train)

            y_pred = predict_with_batches(model, X_test, batch_size=batch_size)
        else:
            # For smaller datasets, we can optimize and train on the full dataset
            if optimize:
                # Define range of n_estimators values to try
                n_estimators_range = [50, 100, 150, 200, 250, 300]

                # Optimize n_estimators
                model, best_n_estimators = optimize_iterations(
                    model, 'n_estimators', n_estimators_range, X_train, y_train
                )
                print(f"Using optimized n_estimators={best_n_estimators} for Gradient Boosting")

            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        is_approximate = False

    else:
        raise ValueError("Unknown model name: {}".format(model_name))

    # Calculate MSE
    mse = mean_squared_error(y_test, y_pred)

    # Calculate training time
    training_time = time.time() - start_time

    print("{} MSE: {:.6f} (Training time: {:.2f}s)".format(model_name, mse, training_time))

    return {
        'model': model,
        'mse': mse,
        'scaler': scaler,
        'is_approximate': is_approximate,
        'training_time': training_time
    }

def predict_with_loaded_model(loaded_data, X_sample, batch_size=1000):
    """
    Make predictions using a loaded model, handling different model types and large datasets.

    Args:
        loaded_data: Dictionary containing the loaded model and components
        X_sample: Input data for prediction
        batch_size: Batch size for large datasets

    Returns:
        predictions: Model predictions
    """


    model_name = loaded_data.get('model_name', 'Unknown')
    is_complex = loaded_data.get('is_complex', False)

    # Handle complex models
    if is_complex:
        return predict_with_complex_model(loaded_data, X_sample, batch_size)

    # Handle regular (non-complex) models
    model = loaded_data['model']
    is_approximate = loaded_data.get('is_approximate', False)
    scaler = loaded_data.get('scaler', None)

    # Handle large input arrays
    if len(X_sample) > batch_size:
        print("Making predictions on large input array (size: {})...".format(len(X_sample)))

        if is_approximate and model_name == 'Gaussian Process':
            # Check if model is already a GaussianProcessApproximation object
            if hasattr(model, 'predict') and not isinstance(model, dict):
                # Use the model's predict method directly
                # Process in batches to avoid memory issues
                predictions = np.zeros(len(X_sample))
                for start_idx in range(0, len(X_sample), batch_size):
                    end_idx = min(start_idx + batch_size, len(X_sample))
                    X_batch = X_sample[start_idx:end_idx]

                    # Scale if needed
                    if scaler is not None:
                        # Check if the scaler is fitted before using transform
                        if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                            X_batch = scaler.transform(X_batch)
                        else:
                            # If scaler is not fitted, fit it on this batch first
                            X_batch = scaler.fit_transform(X_batch)

                    # Use model's predict method
                    predictions[start_idx:end_idx] = model.predict(X_batch)
            else:
                # For backward compatibility with saved models that use dictionary format
                rbf_feature = model['rbf_feature']
                sgd_model = model['sgd_model']

                # Process in batches to avoid memory issues
                predictions = np.zeros(len(X_sample))
                for start_idx in range(0, len(X_sample), batch_size):
                    end_idx = min(start_idx + batch_size, len(X_sample))
                    X_batch = X_sample[start_idx:end_idx]

                    # Scale if needed
                    if scaler is not None:
                        # Check if the scaler is fitted before using transform
                        if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                            X_batch = scaler.transform(X_batch)
                        else:
                            # If scaler is not fitted, fit it on this batch first
                            X_batch = scaler.fit_transform(X_batch)

                    # Transform and predict
                    X_batch_features = rbf_feature.transform(X_batch)
                    predictions[start_idx:end_idx] = sgd_model.predict(X_batch_features)

            return predictions
        else:
            # Use batch prediction for other models
            return predict_with_batches(model, X_sample, batch_size=batch_size, scaler=scaler)
    else:
        # For small inputs, we can predict directly
        if is_approximate and model_name == 'Gaussian Process':
            # Check if model is already a GaussianProcessApproximation object
            if hasattr(model, 'predict') and not isinstance(model, dict):
                # Use the model's predict method directly
                # Scale if needed
                if scaler is not None:
                    # Check if the scaler is fitted before using transform
                    if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                        X_sample = scaler.transform(X_sample)
                    else:
                        # If scaler is not fitted, fit it on this sample first
                        X_sample = scaler.fit_transform(X_sample)

                # Use model's predict method
                return model.predict(X_sample)
            else:
                # For backward compatibility with saved models that use dictionary format
                rbf_feature = model['rbf_feature']
                sgd_model = model['sgd_model']

                # Scale if needed
                if scaler is not None:
                    # Check if the scaler is fitted before using transform
                    if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                        X_sample = scaler.transform(X_sample)
                    else:
                        # If scaler is not fitted, fit it on this sample first
                        X_sample = scaler.fit_transform(X_sample)

                X_sample_features = rbf_feature.transform(X_sample)
                return sgd_model.predict(X_sample_features)
        else:
            # Scale if needed
            if scaler is not None:
                # Check if the scaler is fitted before using transform
                if hasattr(scaler, 'n_features_in_') or hasattr(scaler, 'n_samples_seen_'):
                    X_sample = scaler.transform(X_sample)
                else:
                    # If scaler is not fitted, fit it on this sample first
                    X_sample = scaler.fit_transform(X_sample)

            return model.predict(X_sample)

def predict_with_variable_combinations(loaded_data, variable_lists, batch_size=1000, verbose=True):
    """
    Make predictions for all combinations of input variables.

    Args:
        loaded_data: Dictionary containing the loaded model and components
        variable_lists: List of lists, where each inner list contains possible values for one dimension
        batch_size: Batch size for processing large numbers of combinations
        verbose: Whether to display progress

    Returns:
        results: Dictionary mapping input combinations to predictions
    """
    # Validate input
    if not isinstance(variable_lists, list) or not all(isinstance(var_list, list) for var_list in variable_lists):
        raise ValueError("variable_lists must be a list of lists")

    # Get the number of dimensions
    n_dims = len(variable_lists)

    # Generate all combinations using itertools.product
    all_combinations = list(itertools.product(*variable_lists))
    n_combinations = len(all_combinations)

    if verbose:
        print("Generating predictions for {} combinations of input variables...".format(n_combinations))

    # Process combinations in batches to handle large numbers efficiently
    results = {}

    # Process in batches
    for batch_start in range(0, n_combinations, batch_size):
        batch_end = min(batch_start + batch_size, n_combinations)
        batch_combinations = all_combinations[batch_start:batch_end]

        # Convert combinations to numpy array for prediction
        X_batch = np.array(batch_combinations)

        # Make predictions for this batch
        batch_predictions = predict_with_loaded_model(loaded_data, X_batch, batch_size)

        # Store results
        for i, combination in enumerate(batch_combinations):
            results[combination] = batch_predictions[i]

        if verbose and batch_end < n_combinations:
            print("Processed {}/{} combinations...".format(batch_end, n_combinations))

    if verbose:
        print("Completed predictions for all {} combinations.".format(n_combinations))

    return results

def main(optimize=True, use_complex=True):
    """
    Main function that organizes all inputs and returns outputs.
    This function handles:
    1. Data preparation (including complex data if use_complex=True)
    2. Model training and evaluation
    3. Finding the best model
    4. Saving the model
    5. Example usage of the model

    Args:
        optimize: Whether to optimize the number of iterations or estimators for each model
        use_complex: Whether to use complex arrays (if False, only real parts are used)

    Returns:
        dict: Results including best model, predictions, and combination results
    """
    # Input data - create data with real and optionally imaginary parts

    # Base values for X and y (real parts)
    y_base_real = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6])
    X_base_real = np.array([
        [2, 3, 4, 5, 6, 7],
        [4, 5, 6, 7, 3, 2],
        [5, 6, 7, 2, 1, 3],
        [6, 7, 2, 1, 2, 4],
        [7, 2, 1, 2, 3, 5],
        [2, 1, 2, 3, 4, 6],
        [1, 2, 3, 4, 5, 7],
        [2, 3, 4, 5, 6, 8],
        [3, 4, 5, 6, 7, 9],
        [4, 5, 6, 7, 8, 10],
        [5, 4, 3, 2, 1, 6]
    ])

    if use_complex:
        # Base values for X and y (imaginary parts)
        y_base_imag = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 3.0])
        X_base_imag = np.array([
            [1.0, 1.5, 2.0, 2.5, 3.0, 3.5],
            [2.0, 2.5, 3.0, 3.5, 1.5, 1.0],
            [2.5, 3.0, 3.5, 1.0, 0.5, 1.5],
            [3.0, 3.5, 1.0, 0.5, 1.0, 2.0],
            [3.5, 1.0, 0.5, 1.0, 1.5, 2.5],
            [1.0, 0.5, 1.0, 1.5, 2.0, 3.0],
            [0.5, 1.0, 1.5, 2.0, 2.5, 3.5],
            [1.0, 1.5, 2.0, 2.5, 3.0, 4.0],
            [1.5, 2.0, 2.5, 3.0, 3.5, 4.5],
            [2.0, 2.5, 3.0, 3.5, 4.0, 5.0],
            [2.5, 2.0, 1.5, 1.0, 0.5, 3.0]
        ])

        # Create complex base arrays
        X_base = X_base_real + 1j * X_base_imag
        y_base = y_base_real + 1j * y_base_imag
    else:
        # Use only real parts
        X_base = X_base_real
        y_base = y_base_real

    # Create a larger dataset by repeating the base arrays to enable batch processing
    # Repeat the data to create more than 10,000 samples for X_train after the train-test split
    repeat_factor = 100  # This will create 16,500 samples (with ~11,550 in X_train after 70-30 split)
    X_data = np.tile(X_base, (repeat_factor, 1))
    y_data = np.tile(y_base, repeat_factor)

    if use_complex:
        print(f"Created complex dataset with {len(X_data)} samples to enable batch processing")
    else:
        print(f"Created real dataset with {len(X_data)} samples to enable batch processing")

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X_data, y_data, test_size=0.3, random_state=42)

    # Dictionary to store models and their MSE scores
    models = {}

    # Determine if we should use batch processing based on dataset size
    use_batch_processing = len(X_train) > 10000
    batch_size = 10000 if use_batch_processing else None

    # Set phase preservation flag
    preserve_phase = True  # Set to True to maintain phase-preserving models

    if use_complex:
        print(f"Training complex models with phase preservation: {preserve_phase}")
    else:
        print(f"Training real models")

    # For small datasets, we can train all models sequentially
    if not use_batch_processing:
        print("NOT BATCHING THIS SHIFT")
        # Train models sequentially
        if use_complex:
            # Train complex models
            models['Gaussian Process'] = train_complex_model(
                'Gaussian Process', X_train, y_train, X_test, y_test,
                preserve_phase=preserve_phase, optimize=optimize)
            models['RBF SVR'] = train_complex_model(
                'RBF SVR', X_train, y_train, X_test, y_test, 
                preserve_phase=preserve_phase, optimize=optimize)
            models['Linear SVR'] = train_complex_model(
                'Linear SVR', X_train, y_train, X_test, y_test, 
                preserve_phase=preserve_phase, optimize=optimize)
            models['Neural Network'] = train_complex_model(
                'Neural Network', X_train, y_train, X_test, y_test, 
                preserve_phase=preserve_phase, optimize=optimize)
            models['Random Forest'] = train_complex_model(
                'Random Forest', X_train, y_train, X_test, y_test, 
                use_scaling=False, preserve_phase=preserve_phase, optimize=optimize)
            models['Gradient Boosting'] = train_complex_model(
                'Gradient Boosting', X_train, y_train, X_test, y_test, 
                use_scaling=False, preserve_phase=preserve_phase, optimize=optimize)
        else:
            # Train real models
            models['Gaussian Process'] = train_model(
                'Gaussian Process', X_train, y_train, X_test, y_test,
                use_scaling=True, optimize=optimize)
            models['RBF SVR'] = train_model(
                'RBF SVR', X_train, y_train, X_test, y_test, 
                use_scaling=True, optimize=optimize)
            models['Linear SVR'] = train_model(
                'Linear SVR', X_train, y_train, X_test, y_test, 
                use_scaling=True, optimize=optimize)
            models['Neural Network'] = train_model(
                'Neural Network', X_train, y_train, X_test, y_test, 
                use_scaling=True, optimize=optimize)
            models['Random Forest'] = train_model(
                'Random Forest', X_train, y_train, X_test, y_test, 
                use_scaling=False, optimize=optimize)
            models['Gradient Boosting'] = train_model(
                'Gradient Boosting', X_train, y_train, X_test, y_test, 
                use_scaling=False, optimize=optimize)
    else:
        # For large datasets, train models in parallel
        print("Training models in parallel for large dataset (size: {})...".format(len(X_train)))
        print(" BATCHING THIS SHIFT")
        # Define model configurations
        model_configs = [
            ('Gaussian Process', True),  # (model_name, use_scaling)
            ('RBF SVR', True),
            ('Linear SVR', True),
            ('Neural Network', True),
            ('Random Forest', False),
            ('Gradient Boosting', False)
        ]

        # Train models in parallel
        n_jobs = max(1, multiprocessing.cpu_count() - 1)  # Use all but one CPU
        if use_complex:
            # Train complex models
            results = Parallel(n_jobs=n_jobs)(
                delayed(train_complex_model)(
                    name, X_train, y_train, X_test, y_test,
                    use_scaling, batch_size, preserve_phase, optimize)
                for name, use_scaling in model_configs
            )
        else:
            # Train real models
            results = Parallel(n_jobs=n_jobs)(
                delayed(train_model)(
                    name, X_train, y_train, X_test, y_test,
                    use_scaling, batch_size, optimize)
                for name, use_scaling in model_configs
            )

        # Store results
        for (name, _), result in zip(model_configs, results):
            models[name] = result

    # Find the best model based on MSE
    best_model_name = min(models, key=lambda k: models[k]['mse'])
    best_model_info = models[best_model_name]
    best_mse = best_model_info['mse']
    is_complex = use_complex  # Set based on whether we're using complex arrays
    preserve_phase = best_model_info.get('preserve_phase', True) if is_complex else False
    training_time = best_model_info.get('training_time', 0)

    print("\nBest model: {} with MSE: {:.6f}".format(best_model_name, best_mse))
    print("Training time: {:.2f}s".format(training_time))
    print("Phase preservation: {}".format("Enabled" if preserve_phase else "Disabled"))

    # Save each model with its respective name
    print("\nSaving all models with their respective names...")

    for model_name, model_info in models.items():
        # Create filename based on model name
        if best_model_name == model_name:
            model_filename = f"best_{model_name.replace(' ', '_').lower()}_model.pkl"
        else:
            model_filename = f"{model_name.replace(' ', '_').lower()}_model.pkl"

        # Prepare the data to save
        save_data = {
            'model_name': model_name,
            'is_complex': is_complex,
            'preserve_phase': model_info.get('preserve_phase', True) if is_complex else False
        }

        if is_complex:
            # Add real and imaginary models and components for complex models
            save_data['real_model'] = model_info['real_model']
            save_data['imag_model'] = model_info['imag_model']

            # Add scalers if needed
            if 'real_scaler' in model_info and model_info['real_scaler'] is not None:
                save_data['real_scaler'] = model_info['real_scaler']
            if 'imag_scaler' in model_info and model_info['imag_scaler'] is not None:
                save_data['imag_scaler'] = model_info['imag_scaler']
        else:
            # Add model and scaler for real models
            save_data['model'] = model_info['model']
            if 'scaler' in model_info and model_info['scaler'] is not None:
                save_data['scaler'] = model_info['scaler']

        # Use protocol=2 for pickle to ensure compatibility with older Python versions
        with open(model_filename, 'wb') as file:
            pickle.dump(save_data, file, protocol=2)
        if is_complex:
            print(f"Complex model '{model_name}' saved to {model_filename}")
        else:
            print(f"Real model '{model_name}' saved to {model_filename}")

    # Example of how to load and use the saved model
    if is_complex:
        print("\nExample of loading and using the saved complex model:")
    else:
        print("\nExample of loading and using the saved real model:")
    try:
        with open(model_filename, 'rb') as file:
            # Use encoding='latin1' for compatibility with older Python versions
            loaded_data = pickle.load(file, encoding='latin1')
    except TypeError:
        # If encoding parameter is not supported (Python 2 compatibility)
        with open(model_filename, 'rb') as file:
            loaded_data = pickle.load(file)

    # Make a prediction with the loaded model
    sample_data = X_test[0:1]  # Take the first test sample
    prediction = predict_with_loaded_model(loaded_data, sample_data)

    print("Actual value: {}".format(y_test[0]))
    print("Predicted value: {}".format(prediction[0]))

    if is_complex:
        print("Actual magnitude: {:.4f}, phase: {:.4f}".format(
            np.abs(y_test[0]), np.angle(y_test[0])))
        print("Predicted magnitude: {:.4f}, phase: {:.4f}".format(
            np.abs(prediction[0]), np.angle(prediction[0])))


    # Define lists of possible values for each dimension
    if is_complex:
        # For complex models, use complex values
        # For this example, we'll use 2 possible complex values for each of the 6 dimensions
        dimension_values = [
            [2.0 + 1.0j, 4.0 + 2.0j],  # Possible values for dimension 1
            [3.0 + 1.5j, 5.0 + 2.5j],  # Possible values for dimension 2
            [4.0 + 2.0j, 6.0 + 3.0j],  # Possible values for dimension 3
            [5.0 + 2.5j, 7.0 + 3.5j],  # Possible values for dimension 4
            [6.0 + 3.0j, 8.0 + 4.0j],  # Possible values for dimension 5
            [7.0 + 3.5j, 9.0 + 4.5j]   # Possible values for dimension 6
        ]
    else:
        # For real models, use real values
        # For this example, we'll use 2 possible real values for each of the 6 dimensions
        dimension_values = [
            [2.0, 4.0],  # Possible values for dimension 1
            [3.0, 5.0],  # Possible values for dimension 2
            [4.0, 6.0],  # Possible values for dimension 3
            [5.0, 7.0],  # Possible values for dimension 4
            [6.0, 8.0],  # Possible values for dimension 5
            [7.0, 9.0]   # Possible values for dimension 6
        ]

    # Get predictions for all combinations
    combination_results = predict_with_variable_combinations(loaded_data, dimension_values)

    # Display a few results
    print("\nTotal combinations: {}".format(len(combination_results)))
    print("\nSample predictions:")
    for i, (combination, prediction) in enumerate(combination_results.items()):
        if is_complex:
            print("Input: {}  Prediction: {:.4f}{:+.4f}j (Magnitude: {:.4f}, Phase: {:.4f})".format(
                combination, prediction.real, prediction.imag, np.abs(prediction), np.angle(prediction)))
        else:
            print("Input: {}  Prediction: {:.4f}".format(combination, prediction))
        if i >= 4:  # Show only first 5 combinations
            remaining = len(combination_results) - 5
            print("... and {} more combinations".format(remaining))
            break

    # Return results
    return {
        'best_model_name': best_model_name,
        'best_model_info': best_model_info,
        'best_mse': best_mse,
        'prediction': prediction,
        'combination_results': combination_results
    }

# Call the main function if this script is run directly
if __name__ == "__main__":
    main()
